{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAP 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process data\n",
    "df = pd.read_csv(\"../DAP2/processeddata/aapl.us.csv\", index_col=0)\n",
    "\n",
    "nextDay = df['Open']\n",
    "\n",
    "df['NextDayOpen'] = nextDay.shift(-1)\n",
    "df.drop(df.tail(1).index,inplace=True) # Drop Last Row\n",
    "\n",
    "X = df.drop(columns='NextDayOpen')\n",
    "y = df['NextDayOpen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "X_train = X_train.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "y_test = y_test.to_numpy()\n",
    "\n",
    "# scale data\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Polynomial data\n",
    "\n",
    "kf = KFold(n_splits=10, random_state=None, shuffle=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-Parameter Finding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression Model with Feature Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_range = [0, 1, 10, 20, 50, 100, 1000]\n",
    "\n",
    "for alpha in alpha_range:\n",
    "    # Preform K-Fold cross validation\n",
    "    # Get average of R2 train and test, and RMSE\n",
    "    \n",
    "    r2_train_scores = []\n",
    "    r2_val_scores = []\n",
    "    rmse_scores = []\n",
    "    \n",
    "    for train_index, val_index in kf.split(X_train_scaled):    \n",
    "        cv_X_train = X_train_scaled[train_index]\n",
    "        cv_X_val   = X_train_scaled[val_index]\n",
    "        cv_y_train = y_train[train_index]\n",
    "        cv_y_val   = y_train[val_index]\n",
    "        \n",
    "        linridge = Ridge(alpha = alpha).fit(cv_X_train, cv_y_train)\n",
    "        r2_train = linridge.score(cv_X_train, cv_y_train)\n",
    "        r2_val = linridge.score(cv_X_val, cv_y_val)\n",
    "         \n",
    "        r2_train_scores.append(r2_train)\n",
    "        r2_val_scores.append(r2_val)\n",
    "        \n",
    "        y_pred = linridge.predict(cv_X_val)\n",
    "        rmse_scores.append(np.sqrt(metrics.mean_squared_error(cv_y_val, y_pred)))\n",
    "    \n",
    "    np.mean(r2_train_scores)\n",
    "    np.mean(r2_val_scores)\n",
    "    np.mean(rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression Model with Feature Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_range = [0.5, 1, 2, 3, 5, 10, 20, 50]\n",
    "\n",
    "for alpha in alpha_range:\n",
    "    # Preform K-Fold cross validation\n",
    "    # Get average of R2 train and test, and RMSE\n",
    "    \n",
    "    r2_train_scores = []\n",
    "    r2_val_scores = []\n",
    "    rmse_scores = []\n",
    "    \n",
    "    for train_index, val_index in kf.split(X_train_scaled):    \n",
    "        cv_X_train = X_train_scaled[train_index]\n",
    "        cv_X_val   = X_train_scaled[val_index]\n",
    "        cv_y_train = y_train[train_index]\n",
    "        cv_y_val   = y_train[val_index]\n",
    "        \n",
    "        linlasso = Lasso(alpha, max_iter = 10000).fit(cv_X_train, cv_y_train)\n",
    "        r2_train = linlasso.score(cv_X_train, cv_y_train)\n",
    "        r2_val = linlasso.score(cv_X_val, cv_y_val)\n",
    "         \n",
    "        r2_train_scores.append(r2_train)\n",
    "        r2_val_scores.append(r2_val)\n",
    "        \n",
    "        y_pred = linlasso.predict(cv_X_val)\n",
    "        rmse_scores.append(np.sqrt(metrics.mean_squared_error(cv_y_val, y_pred)))\n",
    "    \n",
    "    np.mean(r2_train_scores)\n",
    "    np.mean(r2_val_scores)\n",
    "    np.mean(rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_range = range(2, 5)\n",
    "\n",
    "for degree in degree_range:\n",
    "    \n",
    "    r2_train_scores = []\n",
    "    r2_val_scores = []\n",
    "    rmse_scores = []\n",
    "    \n",
    "    for train_index, val_index in kf.split(X_train_scaled): # Scaled?\n",
    "        cv_X_train = X_train_scaled[train_index]\n",
    "        cv_X_val   = X_train_scaled[val_index]\n",
    "        cv_y_train = y_train[train_index]\n",
    "        cv_y_val   = y_train[val_index]\n",
    "        \n",
    "        poly_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "        cv_X_train_poly = poly_features.fit_transform(cv_X_train)\n",
    "        cv_X_val_poly = poly_features.fit_transform(cv_X_val)\n",
    "        \n",
    "        polyreg = LinearRegression().fit(cv_X_train_poly, cv_y_train)\n",
    "        \n",
    "        r2_train = polyreg.score(cv_X_train_poly, cv_y_train)\n",
    "        r2_val = polyreg.score(cv_X_val_poly, cv_y_val)\n",
    "         \n",
    "        r2_train_scores.append(r2_train)\n",
    "        r2_val_scores.append(r2_val)\n",
    "        \n",
    "        y_pred = polyreg.predict(cv_X_val_poly)\n",
    "        rmse_scores.append(np.sqrt(metrics.mean_squared_error(cv_y_val, y_pred)))\n",
    "    \n",
    "    np.mean(r2_train_scores)\n",
    "    np.mean(r2_val_scores)\n",
    "    np.mean(rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split X, y again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "r2_train_score = linreg.score(X_train, y_train)\n",
    "r2_test_score = linreg.score(X_test, y_test)\n",
    "\n",
    "y_pred = linreg.predict(X_test)\n",
    "rmse_score = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Linear Regression Model with Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.62572781e+21 2.62572781e+21 2.69367971e+21 ... 2.69367971e+21\n",
      "  2.62572781e+21 2.62572781e+21]\n",
      " [1.45777389e+21 1.45777389e+21 1.49550000e+21 ... 1.49550000e+21\n",
      "  1.45777389e+21 1.45777389e+21]\n",
      " [1.23444930e+21 1.23444930e+21 1.26639593e+21 ... 1.26639593e+21\n",
      "  1.23444930e+21 1.23444930e+21]\n",
      " ...\n",
      " [3.22348367e+21 3.22348367e+21 3.30690504e+21 ... 3.30690504e+21\n",
      "  3.22348367e+21 3.22348367e+21]\n",
      " [1.71944099e+21 1.71944099e+21 1.76393885e+21 ... 1.76393885e+21\n",
      "  1.71944099e+21 1.71944099e+21]\n",
      " [2.87708079e+21 2.87708079e+21 2.95153751e+21 ... 2.95153751e+21\n",
      "  2.87708079e+21 2.87708079e+21]]\n"
     ]
    }
   ],
   "source": [
    "# Not working needs debugging\n",
    "learning_rate = 0.05\n",
    "n_iterations = 10\n",
    "n = len(X_train)\n",
    "theta = np.random.randn(8,1) # Replace 8 with features number\n",
    "        \n",
    "X_b = np.c_[np.ones((len(X_train), 1)), X_train] # Training\n",
    "X_new_b = np.c_[np.ones((len(X_test), 1)), X_test] # Test\n",
    "        \n",
    "for iteration in range(n_iterations): # Train Model\n",
    "    gradients = 2/n * X_b.T.dot(X_b.dot(theta) - y_train)\n",
    "    theta = theta - learning_rate * gradients\n",
    "\n",
    "y_pred = X_new_b.dot(theta) # Prediction of theta for each feature\n",
    "\n",
    "print (y_pred)\n",
    "\n",
    "#y_test_b = y_test\n",
    "#y_test_b.reshape(409, 1)\n",
    "\n",
    "#rmse_score = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "#print(rmse_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression Model with Feature Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linridge = Ridge(alpha = alpha).fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression Model with Feature Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linlasso = Lasso(alpha, max_iter = 10000).fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "X_train_poly = poly_features.fit_transform(X_train)\n",
    "X_test_poly = poly_features.fit_transform(X_test)\n",
    "        \n",
    "polyreg = LinearRegression().fit(X_train_poly, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# R^2\n",
    "\n",
    "print('R-squared score (training): {:.3f}'.format(linreg.score(X_train, y_train)))\n",
    "# Do testing too\n",
    "# RMSE\n",
    "\n",
    "print('RMSE: {:.3f}'.format(np.sqrt(metrics.mean_squared_error(y_test, y_pred))))\n",
    "\n",
    "# Plot learning curves (training=validation RMSE plots)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
