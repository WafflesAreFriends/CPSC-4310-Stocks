{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAP 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab data and split\n",
    "df = pd.read_csv(\"../DAP2/processeddata/2018_Financial_Data.csv\", index_col=0)\n",
    "target_cols = ['priceCashFlowRatio', \n",
    "               'priceEarningsRatio', \n",
    "               'priceEarningsToGrowthRatio', \n",
    "               'priceBookValueRatio', \n",
    "               'currentRatio', \n",
    "               'quickRatio',\n",
    "               'payoutRatio']\n",
    "X = df[target_cols]\n",
    "y = df['Class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "X_train = X_train.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "y_test = y_test.to_numpy()\n",
    "\n",
    "# scale data\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Polynomial data\n",
    "\n",
    "kf = KFold(n_splits=10, random_state=None, shuffle=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-Parameter Finding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression Model with Feature Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_range = [0, 1, 10, 20, 50, 100, 1000]\n",
    "\n",
    "for alpha in alpha_range:\n",
    "    # Preform K-Fold cross validation\n",
    "    # Get average of R2 train and test, and RMSE\n",
    "    \n",
    "    r2_train_scores = []\n",
    "    r2_val_scores = []\n",
    "    rmse_scores = []\n",
    "    \n",
    "    for train_index, val_index in kf.split(X_train_scaled):    \n",
    "        cv_X_train = X_train_scaled[train_index]\n",
    "        cv_X_val   = X_train_scaled[val_index]\n",
    "        cv_y_train = y_train[train_index]\n",
    "        cv_y_val   = y_train[val_index]\n",
    "        \n",
    "        linridge = Ridge(alpha = alpha).fit(cv_X_train, cv_y_train)\n",
    "        r2_train = linridge.score(cv_X_train, cv_y_train)\n",
    "        r2_val = linridge.score(cv_X_val, cv_y_val)\n",
    "         \n",
    "        r2_train_scores.append(r2_train)\n",
    "        r2_val_scores.append(r2_val)\n",
    "        \n",
    "        y_pred = linridge.predict(cv_X_val)\n",
    "        rmse_scores.append(np.sqrt(metrics.mean_squared_error(cv_y_val, y_pred)))\n",
    "    \n",
    "    np.mean(r2_train_scores)\n",
    "    np.mean(r2_val_scores)\n",
    "    np.mean(rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression Model with Feature Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_range = [0.5, 1, 2, 3, 5, 10, 20, 50]\n",
    "\n",
    "for alpha in alpha_range:\n",
    "    # Preform K-Fold cross validation\n",
    "    # Get average of R2 train and test, and RMSE\n",
    "    \n",
    "    r2_train_scores = []\n",
    "    r2_val_scores = []\n",
    "    rmse_scores = []\n",
    "    \n",
    "    for train_index, val_index in kf.split(X_train_scaled):    \n",
    "        cv_X_train = X_train_scaled[train_index]\n",
    "        cv_X_val   = X_train_scaled[val_index]\n",
    "        cv_y_train = y_train[train_index]\n",
    "        cv_y_val   = y_train[val_index]\n",
    "        \n",
    "        linlasso = Lasso(alpha, max_iter = 10000).fit(cv_X_train, cv_y_train)\n",
    "        r2_train = linlasso.score(cv_X_train, cv_y_train)\n",
    "        r2_val = linlasso.score(cv_X_val, cv_y_val)\n",
    "         \n",
    "        r2_train_scores.append(r2_train)\n",
    "        r2_val_scores.append(r2_val)\n",
    "        \n",
    "        y_pred = linlasso.predict(cv_X_val)\n",
    "        rmse_scores.append(np.sqrt(metrics.mean_squared_error(cv_y_val, y_pred)))\n",
    "    \n",
    "    np.mean(r2_train_scores)\n",
    "    np.mean(r2_val_scores)\n",
    "    np.mean(rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_range = range(2, 5)\n",
    "\n",
    "for degree in degree_range:\n",
    "    \n",
    "    r2_train_scores = []\n",
    "    r2_val_scores = []\n",
    "    rmse_scores = []\n",
    "    \n",
    "    for train_index, val_index in kf.split(X_train_scaled): # Scaled?\n",
    "        cv_X_train = X_train_scaled[train_index]\n",
    "        cv_X_val   = X_train_scaled[val_index]\n",
    "        cv_y_train = y_train[train_index]\n",
    "        cv_y_val   = y_train[val_index]\n",
    "        \n",
    "        poly_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "        cv_X_train_poly = poly_features.fit_transform(cv_X_train)\n",
    "        cv_X_val_poly = poly_features.fit_transform(cv_X_val)\n",
    "        \n",
    "        polyreg = LinearRegression().fit(cv_X_train_poly, cv_y_train)\n",
    "        \n",
    "        r2_train = polyreg.score(cv_X_train_poly, cv_y_train)\n",
    "        r2_val = polyreg.score(cv_X_val_poly, cv_y_val)\n",
    "         \n",
    "        r2_train_scores.append(r2_train)\n",
    "        r2_val_scores.append(r2_val)\n",
    "        \n",
    "        y_pred = polyreg.predict(cv_X_val_poly)\n",
    "        rmse_scores.append(np.sqrt(metrics.mean_squared_error(cv_y_val, y_pred)))\n",
    "    \n",
    "    np.mean(r2_train_scores)\n",
    "    np.mean(r2_val_scores)\n",
    "    np.mean(rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split X, y again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "r2_train_score = linreg.score(X_train, y_train)\n",
    "r2_test_score = linreg.score(X_test, y_test)\n",
    "\n",
    "y_pred = linreg.predict(X_test)\n",
    "rmse_score = np.sqrt(metrics.mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Linear Regression Model with Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y_true and y_pred have different number of output (1!=1227)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-5f07ff2301f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0my_test_b\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m409\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mrmse_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;31m#print(rmse_score)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/metrics/regression.py\u001b[0m in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \"\"\"\n\u001b[1;32m    240\u001b[0m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[0;32m--> 241\u001b[0;31m         y_true, y_pred, multioutput)\n\u001b[0m\u001b[1;32m    242\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/metrics/regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         raise ValueError(\"y_true and y_pred have different number of output \"\n\u001b[0;32m---> 89\u001b[0;31m                          \"({0}!={1})\".format(y_true.shape[1], y_pred.shape[1]))\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0mn_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: y_true and y_pred have different number of output (1!=1227)"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.05\n",
    "n_iterations = 10\n",
    "n = len(cv_X_train)\n",
    "theta = np.random.randn(8,1) # Replace 8 with features number\n",
    "        \n",
    "X_b = np.c_[np.ones((len(X_train), 1)), X_train] # Training\n",
    "X_new_b = np.c_[np.ones((len(X_test), 1)), X_test] # Test\n",
    "        \n",
    "for iteration in range(n_iterations): # Train Model\n",
    "    gradients = 2/n * X_b.T.dot(X_b.dot(theta) - y_train)\n",
    "    theta = theta - learning_rate * gradients\n",
    "\n",
    "y_pred = X_new_b.dot(theta) # Prediction of theta for each feature\n",
    "\n",
    "#y_test_b = y_test\n",
    "#y_test_b.reshape(409, 1)\n",
    "\n",
    "rmse_score = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "#print(rmse_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'builtin_function_or_method' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-53365aa06aa6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX_new_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_new\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# add x0 = 1 to each instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_new_b\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'builtin_function_or_method' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "X_new = np.array([[0], [2]])\n",
    "X_new_b = np.c_[np.ones((2, 1)), X_new] # add x0 = 1 to each instance\n",
    "y_predict = X_new_b.dot(theta)\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression Model with Feature Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linridge = Ridge(alpha = alpha).fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression Model with Feature Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linlasso = Lasso(alpha, max_iter = 10000).fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "X_train_poly = poly_features.fit_transform(X_train)\n",
    "X_test_poly = poly_features.fit_transform(X_test)\n",
    "        \n",
    "polyreg = LinearRegression().fit(X_train_poly, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# R^2\n",
    "\n",
    "print('R-squared score (training): {:.3f}'.format(linreg.score(X_train, y_train)))\n",
    "# Do testing too\n",
    "\n",
    "\n",
    "# RMSE\n",
    "\n",
    "print('RMSE: {:.3f}'.format(np.sqrt(metrics.mean_squared_error(y_test, y_pred))))\n",
    "\n",
    "# Plot learning curves (training=validation RMSE plots)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
